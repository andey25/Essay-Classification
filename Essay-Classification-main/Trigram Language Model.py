# -*- coding: utf-8 -*-
"""NLP_HW_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11LQGxSDXxwkHzm4yXnMJ2xO3MU01OtOc
"""

import sys
from collections import defaultdict
import math
import random
import os
import os.path
"""
COMS W4705 - Natural Language Processing - Fall 2023
Programming Homework 1 - Trigram Language Models
Daniel Bauer
"""

def corpus_reader(corpusfile, lexicon=None):
    with open(corpusfile,'r') as corpus:
        for line in corpus:
            if line.strip():
                sequence = line.lower().strip().split()
                if lexicon:
                    yield [word if word in lexicon else "UNK" for word in sequence]
                else:
                    yield sequence

def get_lexicon(corpus):
    word_counts = defaultdict(int)
    for sentence in corpus:
        for word in sentence:
            word_counts[word] += 1
    return set(word for word in word_counts if word_counts[word] > 1)

def get_ngrams(sequence, n):
    """
    Given a sequence (list of strings) and an integer n, this function returns a list of n-grams.
    Each n-gram is a Python tuple. The sequence is padded with 'START' and 'STOP' tokens to allow
    for the generation of proper n-grams. For n=1, the sequence is prefixed with a single 'START'
    and suffixed with a single 'STOP'. For n>1, the sequence is prefixed with n-1 'START' tokens
    and suffixed with a single 'STOP' token.
    """
    if n > 1:
        padded_sequence = ['START'] * (n-1) + sequence + ['STOP']
    else:
        padded_sequence = ['START'] + sequence + ['STOP']

    ngrams = [tuple(padded_sequence[i:i+n]) for i in range(len(padded_sequence) - n + 1)]
    return ngrams

# Test Cases:
print(get_ngrams(["natural", "language", "processing"], 1))
print(get_ngrams(["natural", "language", "processing"], 2))
print(get_ngrams(["natural", "language", "processing"], 3))

from google.colab import drive
drive.mount('/content/drive')
data_folder_path = '/content/drive/My Drive/hw1_data'

class TrigramModel(object):

    def __init__(self, corpusfile):

        # Iterate through the corpus once to build a lexicon
        generator = corpus_reader(corpusfile)
        self.lexicon = get_lexicon(generator)
        self.lexicon.add("UNK")
        self.lexicon.add("START")
        self.lexicon.add("STOP")

        # Now iterate through the corpus again and count ngrams
        generator = corpus_reader(corpusfile, self.lexicon)
        self.count_ngrams(generator)


    def count_ngrams(self, corpus):
        self.unigramcounts = defaultdict(int)
        self.bigramcounts = defaultdict(int)
        self.trigramcounts = defaultdict(int)

        for sentence in corpus:
            unigrams = get_ngrams(sentence, 1)
            bigrams = get_ngrams(sentence, 2)
            trigrams = get_ngrams(sentence, 3)

            for unigram in unigrams:
                self.unigramcounts[unigram] += 1
            for bigram in bigrams:
                self.bigramcounts[bigram] += 1
            for trigram in trigrams:
                self.trigramcounts[trigram] += 1

        # We can also convert defaultdicts to regular dicts
        self.unigramcounts = dict(self.unigramcounts)
        self.bigramcounts = dict(self.bigramcounts)
        self.trigramcounts = dict(self.trigramcounts)

    def raw_trigram_probability(self,trigram):
        """
        COMPLETE THIS METHOD (PART 3)
        Returns the raw (unsmoothed) trigram probability
        """
        trigram_count = self.trigramcounts.get(trigram, 0)

        # Counting the first two words in the trigram (as a bigram)
        bigram_count = self.bigramcounts.get(trigram[:2], 0)
        if bigram_count == 0:
            return 1 / len(self.lexicon)
        else:
            return trigram_count / bigram_count

    def raw_bigram_probability(self, bigram):
        """
        COMPLETE THIS METHOD (PART 3)
        Returns the raw (unsmoothed) bigram probability
        """
        bigram_count = self.bigramcounts.get(bigram, 0)
        first_word_count = self.unigramcounts.get((bigram[0],), 0)
        return bigram_count / first_word_count if first_word_count > 0 else 0.0

    def raw_unigram_probability(self, unigram):
        """
        COMPLETE THIS METHOD (PART 3)
        Returns the raw (unsmoothed) unigram probability.
        """

        #hint: recomputing the denominator every time the method is called
        # can be slow! You might want to compute the total number of words once,
        # store in the TrigramModel instance, and then re-use it.

        total_word_count = sum(self.unigramcounts.values())
        unigram_count = self.unigramcounts.get(unigram, 0)
        return unigram_count / total_word_count if total_word_count > 0 else 0.0

    def generate_sentence(self,t=20):
        """
        COMPLETE THIS METHOD (OPTIONAL)
        Generate a random sentence from the trigram model. t specifies the
        max length, but the sentence may be shorter if STOP is reached.
        """

        sentence = ['START', 'START']
        for _ in range(t):
            context = (sentence[-2], sentence[-1])

            candidates = [(trigram, self.raw_trigram_probability(trigram)) for trigram in self.trigramcounts if trigram[:2] == context]

            if not candidates:
                break

            total_prob = sum(prob for _, prob in candidates)

            probabilities = [prob / total_prob for _, prob in candidates]
            next_word = random.choices([trigram[0][2] for trigram in candidates], weights=probabilities)[0]

            if next_word == 'STOP':
                break

            sentence.append(next_word)

        return sentence[2:]

        # Check the code cell below to test the method

    def smoothed_trigram_probability(self, trigram):
        """
        COMPLETE THIS METHOD (PART 4)
        Returns the smoothed trigram probability (using linear interpolation).
        """
        lambda1 = 1/3.0
        lambda2 = 1/3.0
        lambda3 = 1/3.0

        p_unigram = self.raw_unigram_probability((trigram[2],))
        p_bigram = self.raw_bigram_probability(trigram[1:])
        p_trigram = self.raw_trigram_probability(trigram)

        # Linear interpolation
        smoothed_prob = lambda1 * p_unigram + lambda2 * p_bigram + lambda3 * p_trigram
        return smoothed_prob


    def sentence_logprob(self, sentence):
        """
        COMPLETE THIS METHOD (PART 5)
        Returns the log probability of an entire sequence.
        """
        trigrams = get_ngrams(sentence, 3)

        log_prob_sum = 0

        # Calculating and summing the log probabilities of each trigram
        for trigram in trigrams:
            smoothed_prob = self.smoothed_trigram_probability(trigram)
            if smoothed_prob > 0:
                log_prob_sum += math.log2(smoothed_prob)
            else:
                log_prob_sum += math.log2(1e-10)

        return log_prob_sum


    def perplexity(self, corpus):
        """
        COMPLETE THIS METHOD (PART 6)
        Returns the log probability of an entire sequence.
        """

        total_log_prob = 0
        total_word_count = 0

        for sentence in corpus:
            total_word_count += len(sentence) + 1  # +1 is added for the STOP token
            log_prob = self.sentence_logprob(sentence)
            total_log_prob += log_prob

        l = total_log_prob / total_word_count
        return 2 ** (-l)

def essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):

        model1 = TrigramModel(training_file1)
        model2 = TrigramModel(training_file2)

        total = 0
        correct = 0

        for f in os.listdir(testdir1):
            pp = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))
            essay_path = os.path.join(testdir1, f)
            essay_corpus = corpus_reader(essay_path, model1.lexicon)
            pp1 = model1.perplexity(essay_corpus)
            essay_corpus = corpus_reader(essay_path, model2.lexicon)  # Reinitialize corpus for model2
            pp2 = model2.perplexity(essay_corpus)

            total += 1
            if pp1 < pp2:
               correct += 1

        for f in os.listdir(testdir2):
            pp = model2.perplexity(corpus_reader(os.path.join(testdir2, f), model2.lexicon))
            essay_path = os.path.join(testdir2, f)
            essay_corpus = corpus_reader(essay_path, model1.lexicon)
            pp1 = model1.perplexity(essay_corpus)
            essay_corpus = corpus_reader(essay_path, model2.lexicon)  # Reinitialize corpus for model2
            pp2 = model2.perplexity(essay_corpus)

            total += 1
            if pp2 < pp1:
               correct += 1

        accuracy = correct / total if total > 0 else 0
        return accuracy
        print (accuracy)

if __name__ == "__main__":

      training_file1 = '/content/drive/My Drive/hw1_data/ets_toefl_data/train_high.txt'
      training_file2 = '/content/drive/My Drive/hw1_data/ets_toefl_data/train_low.txt'
      testdir1 = '/content/drive/My Drive/hw1_data/ets_toefl_data/test_high'
      testdir2 = '/content/drive/My Drive/hw1_data/ets_toefl_data/test_low'

      accuracy = essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2)
      print(f"Accuracy: {accuracy}")

# Testing the raw_trigram_probability, raw_bigram_probability, and raw_unigram_probability methods

if __name__ == "__main__":
    model = TrigramModel('/content/drive/My Drive/hw1_data/ets_toefl_data/train_high.txt')

    # Example n-grams for testing
    test_unigram = ('the',) # Unigram
    test_bigram = ('of', 'the')  # Bigram
    test_trigram = ('START', 'START', 'of')  # trigram

    unigram_prob = model.raw_unigram_probability(test_unigram)
    bigram_prob = model.raw_bigram_probability(test_bigram)
    trigram_prob = model.raw_trigram_probability(test_trigram)

    print(f"Raw unigram probability P('the'): {unigram_prob}")
    print(f"Raw bigram probability P('of', 'the'): {bigram_prob}")
    print(f"Raw trigram probability P('START', 'START', 'of'): {trigram_prob}")

# Testing count_ngrams method


def print_ngram_counts(model, ngrams):
    for ngram in ngrams:
        if len(ngram) == 1:
            print(f"Unigram {ngram}: {model.unigramcounts[ngram]}")
        elif len(ngram) == 2:
            print(f"Bigram {ngram}: {model.bigramcounts[ngram]}")
        elif len(ngram) == 3:
            print(f"Trigram {ngram}: {model.trigramcounts[ngram]}")
    print("\n")

if __name__ == "__main__":

    model = TrigramModel('/content/drive/My Drive/hw1_data/ets_toefl_data/train_high.txt')

    # Example n-grams to check
    test_ngrams = [
        ('the',),  # Unigram
        ('START', 'the'),  # Bigram
        ('START', 'START', 'the')  # Trigram
    ]

    print_ngram_counts(model, test_ngrams)

# Testing the generate_sentence method

if __name__ == "__main__":

    training_file = '/content/drive/My Drive/hw1_data/ets_toefl_data/train_high.txt'
    model = TrigramModel(training_file)

    sentence = model.generate_sentence()
    print(' '.join(sentence))

    sentence = model.generate_sentence()
    print(' '.join(sentence))

# Running the perplexity function on the Brown test, brown train and training_high data

model = TrigramModel('/content/drive/My Drive/hw1_data/ets_toefl_data/train_high.txt')

brown_test_corpus = corpus_reader('/content/drive/My Drive/hw1_data/brown_test.txt', model.lexicon)
brown_test_perplexity = model.perplexity(brown_test_corpus)
print(f"Perplexity on the Brown corpus test set: {brown_test_perplexity}")

brown_train_corpus = corpus_reader('/content/drive/My Drive/hw1_data/brown_train.txt', model.lexicon)
brown_train_perplexity = model.perplexity(brown_train_corpus)
print(f"Perplexity on the Brown corpus train set: {brown_train_perplexity}")

training_corpus = corpus_reader('/content/drive/My Drive/hw1_data/ets_toefl_data/train_high.txt', model.lexicon)
training_perplexity = model.perplexity(training_corpus)
print(f"Perplexity on the training data: {training_perplexity}")